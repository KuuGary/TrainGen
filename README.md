# TrainGen: Structured Training Data Generator for Code Repositories

**TrainGen** is an automated training data generation toolkit designed to extract structural and semantic information from local Python codebases, and generate high-quality training samples. It focuses on two key tasks:

1. **Function-level Q&A generation**: Create question-answer pairs with reasoning traces.
2. **Requirement-to-design generation**: Convert natural language requirements into implementation designs with reasoning traces.

---

# üìò Training Data Design Document

This document outlines the data structure and generation strategy for two key training scenarios.

Both scenarios are designed to support fine-tuning or evaluating language models with instruction-following capabilities based on real-world codebases. Sample outputs are provided in the `/sample_results/` folder.

## Scenario 1: Code-based Q&A Generation
 - Each function in `functions.json` and `repo_summary.txt` is used as input to the QA generation module (llm_generate_qa.py).
 - The model is prompted to answer the question: currently fixed to ‚ÄúÂáΩÊï∞ X ÊòØÂÅö‰ªÄ‰πàÁöÑÔºü‚Äù

### Data Format

Each sample is stored as a JSON object in a `.jsonl` file. The structure is as follows:

```
{
  "file": File path to the source code file,
  "code": Full source code of the extracted function.,
  "function_name": Name of the function,
  "question": A natural-language question generated from the function context (currently in Chinese, bilingual support optional),
  "answer": A concise and correct answer grounded in the function's logic and documentation,
  "trace": A multi-step reasoning process
}
```

Example:
```
{
"file": "example/test/conftest.py",
"code": "def pytest_addoption(parser):\n    # ability to test API on different hosts\n    parser.addoption(\"--host\", action=\"store\", default=\"http://localhost:5000\")", "function_name": "pytest_addoption",
"question": "ÂáΩÊï∞ pytest_addoption ÊòØÂÅö‰ªÄ‰πàÁöÑÔºü",
"answer": "ËØ•ÂáΩÊï∞ÁöÑ‰ΩúÁî®ÊòØ‰∏∫ÊµãËØïAPIÊèê‰æõÊîØÊåÅÔºåÂÖÅËÆ∏Âú®‰∏çÂêåÁöÑ hosts‰∏äËøêË°åÊµãËØï„ÄÇÂÆÉÈÄöËøáparserÂØπË±°Ê∑ªÂä†ÈÄâÈ°πÔºåÂπ∂Â∞ÜÈÄâÈ°πÁöÑÂÄº‰º†ÂÖ•Âà∞Ë∞ÉÁî®ÂáΩÊï∞‰∏≠„ÄÇ",
"trace": "È¶ñÂÖàÔºåÂáΩÊï∞‰ªéÂèÇÊï∞ÂàÜÊûêÂºÄÂßãÔºåÂÆÉÊé•Êî∂‰∫Ü‰∏Ä‰∏™ÂèÇÊï∞ 'parser'„ÄÇÁÑ∂ÂêéÔºåÂáΩÊï∞Ê£ÄÊü•ÂèÇÊï∞ÁöÑÁ±ªÂûãÔºåÂπ∂Â∞ÜÂèÇÊï∞‰º†ÈÄíÁªô parser.addoption ÊñπÊ≥ï„ÄÇÊé•‰∏ãÊù•ÔºåÂáΩÊï∞Ê£ÄÊü•ÂèÇÊï∞ÁöÑÁ±ªÂûãÔºåÂπ∂Â∞ÜÂèÇÊï∞ÁöÑÂÄº‰º†ÈÄíÁªô addoption ÊñπÊ≥ï„ÄÇÊúÄÂêéÔºåÂáΩÊï∞Ë∞ÉÁî® addoption ÊñπÊ≥ïÔºåÂπ∂Â∞ÜÂèÇÊï∞ÁöÑÂÄº‰º†ÂÖ•Âà∞ÊñπÊ≥ï‰∏≠„ÄÇ"
}
```

## Scenario 2: Requirement-to-Design Generation
 - The same repo_summary.txt is used to provide architectural context to the model.
 - Currently, 5 requirements are manually written to cover typical backend development tasks such as:
   - Numerical computation API
   - Health check endpoint
   - Logging feature
   - Input mocking
   - Configuration management

### Data Format

Each sample is stored as a JSON object in a `.jsonl` file. The structure is as follows:

```
{
  "requirement": Business requirement or user story,
  "design": A concise design response grounded in the existing code structure,
  "trace": A multi-step reasoning process,
}
```
Example:
```
{
"requirement": "Ê∑ªÂä†‰∏Ä‰∏™Êñ∞ÁöÑ API Êé•Âè£ÔºåÁî®‰∫éËÆ°ÁÆóÁî®Êà∑Êèê‰∫§ÁöÑ‰∏§‰∏™Êï∞ÁöÑÂπ≥ÂùáÂÄº„ÄÇ",
"design": "Âú®ÂêéÁ´ØÊúçÂä°‰∏≠Êñ∞Â¢û‰∏Ä‰∏™ GET Á±ªÂûãÁöÑÊé•Âè£Ôºå‰æãÂ¶Ç `/average`„ÄÇËØ•Êé•Âè£Êé•Êî∂‰∏Ä‰∏™ÂåÖÂê´‰∏§‰∏™Êï¥Êï∞ÁöÑ JSON ËØ∑Ê±Ç‰ΩìÔºåËÆ°ÁÆóÂπ∂ËøîÂõûËøô‰∏§‰∏™Êï∞ÁöÑÂπ≥ÂùáÂÄº„ÄÇÂú®Êé•Âè£ËØ∑Ê±ÇÂ§ÑÁêÜ‰∏≠ÔºåÈúÄË¶ÅÂ∞ÜËØ∑Ê±Ç‰∏≠ÁöÑ‰∏§‰∏™Êï¥Êï∞ÂàÜÂà´ÊèêÂèñÂá∫Êù•Âπ∂ËøõË°åËÆ°ÁÆó„ÄÇÊúÄÁªàÁªìÊûú‰ª• JSON Ê†ºÂºèËøîÂõû„ÄÇ",
"trace": "ËØ•ÂäüËÉΩÂ±û‰∫éÂÖ∏ÂûãÁöÑÊï∞ÂÄºËÆ°ÁÆóÁ±ª‰∏öÂä°ÈÄªËæëÔºåÈÄÇÂêàÈÄöËøá Flask ÁöÑ GET Êé•Âè£Â§ÑÁêÜ„ÄÇËÄÉËôëÂà∞È°πÁõÆÂ∑≤‰ΩøÁî® FlaskÔºåÂèØÈÄöËøá `@app.route('/average', methods=['GET'])` ÊàñËìùÂõæÊñπÂºèÊ≥®ÂÜåË∑ØÁî±„ÄÇËØ∑Ê±Ç‰ΩìÊ†ºÂºèÂ∫î‰∏∫ JSONÔºåÂõ†Ê≠§ÈúÄË¶ÅÈÄöËøá `request.get_json()` Ëé∑ÂèñÊï∞ÊçÆ„ÄÇËÆ°ÁÆóÂπ≥ÂùáÂÄºÂèØÁõ¥Êé•‰ΩøÁî® `(int1 + int2) / 2`ÔºåÁªìÊûú‰ª• JSON Ê†ºÂºèËøîÂõû„ÄÇÊó†ÈúÄÂºïÂÖ•È¢ùÂ§ñ‰æùËµñÔºåËÆæËÆ°‰∏äÂ∫îÂ∞ΩÈáèÂ§çÁî®Â∑≤ÊúâËæìÂÖ•ËæìÂá∫Â§ÑÁêÜÊµÅÁ®ãÔºåÁ°Æ‰øù‰∏ÄËá¥ÊÄßÂíåÂèØÁª¥Êä§ÊÄß„ÄÇ"
}
```

## Metadata Summary

Both scenarios embed contextual metadata to generate its training dataset, summarize as followings:

| Field           | Scenario 1 (Code-based QA) | Scenario 2 (Requirement-to-Design) |
|----------------|----------------------------|------------------------------------|
| `file`         | ‚úÖ Source path              | ‚ùå                                  |
| `code`         | ‚úÖ Function body            | ‚ùå                                  |
| `function_name`| ‚úÖ Function name            | ‚ùå                                  |
| `question`     | ‚úÖ Natural language query   | ‚ùå                                  |
| `requirement`  | ‚ùå                          | ‚úÖ Business requirement             |
| `repo_summary` | ‚úÖ Used during generation   | ‚úÖ Used during generation           |

## Design Rationale

1. Grounded in Real Code Contexts
The data is generated directly from local repositories (via `extractor.py`). This ensures that: All questions and answers are grounded in actual business logic.

3. End-to-End Instruction Format
Each sample follows an instruction ‚Üí reasoning ‚Üí result pattern (e.g., requirement ‚Üí design + trace), which aligns well with the structure expected LLMs.

3. JSONL for Modularity & Automation
Using .jsonl format enables: Easy processing and compatibility with popular fine-tuning frameworks


### I adopt a practical and evolving approach to dataset diversity:

## ‚úÖ Current Measures

 - Scenario 1 traverses functions across all directories to collect varied technical roles.
 - Scenario 2 includes a diverse category mix of requirements.
 - Reasoning traces are encouraged to promote explainability, not just ansIrs.

## üöß Known Limitations

 - Scenario 1 questions are currently uniform and repetitive; lacks dynamic question generation.
 - No semantic filtering is applied ‚Äî trivial or irrelevant functions are still included.
 - Scenario 2 requirements are few and manually written.
 - No multilingual or multi-turn examples yet.


# System Overview

The system is modular and consists of three main components:
```
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    Local Codebase Git  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ extractor.py‚îÇ  ‚Üê Extracts functions, docstrings, README, etc.
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ-‚îÄ‚îê
   ‚îÇ                               ‚îÇ
   ‚ñº generate_qa.py                ‚ñº generate_requirement.py
(Scenario 1ÔºöQA pair)       (Scenario 2Ôºörequirements-to-design)
   ‚îÇ                               ‚îÇ
   ‚ñº                               ‚ñº
 qa_data.jsonl           equirement_data.jsonl
   ‚îÇ                               ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ÜíFinetuning or evaluation ‚Üí‚îò
```
Each module plays a distinct role in the pipeline:

- `extractor.py`: Parses the codebase to extract function signatures, docstrings, and code structure. Outputs:
  - `data/functions.json`: function-level details
  - `data/repo_summary.txt`: human-readable summary including the README

- `llm_generate_qa.py`: Generates Q&A pairs for each function with reasoning steps, based on repo summary and code.

- `llm_generate_requirements.py`: Given a natural language requirement, generates a design description and detailed reasoning based on existing code structure.

# Installation & Usage

## Prerequisites
- Python >= 3.8
- pip
- (Optional) GPU environment if you plan to fine-tune or evaluate models

## Installation
Clone the repository and install dependencies:
```
git clone https://github.com/KuuGary/TrainGen.git
cd TrainGen
pip install -r requirements.txt
```

## Usage

### Generate QA training data (Scenario 1)
```
python main.py --task qa
```
### Generate requirement-to-design training data (Scenario 1)
```
python main.py --task re
```

## Evaluation Strategy (Planned)
I plan to evaluate the data quality via small-scale fine-tuning. A test pipeline has been initialized:
```
/test/
‚îú‚îÄ‚îÄ finetune.py              
‚îú‚îÄ‚îÄ inference.py          
```
However, due to the current lack of a GPU environment and limited compute resources, model fine-tuning and downstream evaluation are not yet completed.

# Other Notes

- A rule-based pipeline (utils/generate_qa.py) allows deterministic generation based on curated heuristics, such as function names and docstring/keyword matches.
- A few-shot LLM-based generation pipeline leverages prompt engineering to produce stable, high-quality samples one at a time, reducing hallucination and ensuring structural consistency.
- The code is modular, lightweight, and requires minimal setup, making it easy to integrate into internal tooling or adapt for other domains.

# üîÑ Future Work & Extensions

This framework is designed to be extensible. Possible directions include:

- Bilingual Support: extendable to support both Chinese and English, or other languages, by modifying prompt templates and LLMs.
- Model Upgrade: While current generation uses DeepSeek-Distill 1.3B or similar models, switching to larger models will likely improve answer fluency, trace reasoning, and design richness.
- Question Diversification: Currently, scenario 1 focuses only on ‚ÄúWhat does this function do?‚Äù. This can be expanded to include, for example: What is the purpose of this parameter? How does this function relate to others in the same module? and so on.
- Currently, all functions are included. Future iterations can: Rank by function complexity or docstring availability. Sample functions representative of different modules or roles.
- Evaluation Pipeline
- Support for More Project Types

